{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pIa27HrmhIMI",
    "outputId": "e73d26e8-ae60-4bb4-9c6f-8095a77b3978"
   },
   "outputs": [],
   "source": [
    "#tfds.as_dataframe(train_df.head())\n",
    "\n",
    "!wget http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz\n",
    "!tar -xvf food-101.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "nh2KT0Upc0c8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers import Dropout, Flatten, Dense, Input, GlobalAveragePooling2D\n",
    "from keras import applications\n",
    "from keras.applications import ResNet152V2\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from tensorflow.keras import regularizers\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger, LearningRateScheduler, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam  \n",
    "import keras,os,re,math,itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend as K\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.optimizers import SGD\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "print(tf.config.experimental.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "K9Eja5uRP_mh"
   },
   "outputs": [],
   "source": [
    "# read data block\n",
    "\n",
    "\n",
    "# Helper method to split dataset into train and test folders\n",
    "def prepare_data(filepath, src,dest):\n",
    "  classes_images = defaultdict(list)\n",
    "  with open(filepath, 'r') as txt:\n",
    "      paths = [read.strip() for read in txt.readlines()]\n",
    "      for p in paths:\n",
    "        food = p.split('/')\n",
    "        classes_images[food[0]].append(food[1] + '.jpg')\n",
    "\n",
    "  for food in classes_images.keys():\n",
    "    print(\"\\nCopying images into \",food)\n",
    "    if not os.path.exists(os.path.join(dest,food)):\n",
    "      os.makedirs(os.path.join(dest,food))\n",
    "    for i in classes_images[food]:\n",
    "      copy(os.path.join(src,food,i), os.path.join(dest,food,i))\n",
    "  print(\"Copying Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZnecwK2lUjUH",
    "outputId": "d0a81371-9ee5-4729-e2ee-0123faec4340"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import collections\n",
    "import os\n",
    "from shutil import copy\n",
    "from shutil import copytree, rmtree\n",
    "prepare_data('food-101/meta/train.txt', 'food-101/images', 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BxYd0UNrVbB7",
    "outputId": "e6054496-95ff-4dc9-85f9-d5d594c087cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Copying images into  apple_pie\n",
      "\n",
      "Copying images into  baby_back_ribs\n",
      "\n",
      "Copying images into  baklava\n",
      "\n",
      "Copying images into  beef_carpaccio\n",
      "\n",
      "Copying images into  beef_tartare\n",
      "\n",
      "Copying images into  beet_salad\n",
      "\n",
      "Copying images into  beignets\n",
      "\n",
      "Copying images into  bibimbap\n",
      "\n",
      "Copying images into  bread_pudding\n",
      "\n",
      "Copying images into  breakfast_burrito\n",
      "\n",
      "Copying images into  bruschetta\n",
      "\n",
      "Copying images into  caesar_salad\n",
      "\n",
      "Copying images into  cannoli\n",
      "\n",
      "Copying images into  caprese_salad\n",
      "\n",
      "Copying images into  carrot_cake\n",
      "\n",
      "Copying images into  ceviche\n",
      "\n",
      "Copying images into  cheesecake\n",
      "\n",
      "Copying images into  cheese_plate\n",
      "\n",
      "Copying images into  chicken_curry\n",
      "\n",
      "Copying images into  chicken_quesadilla\n",
      "\n",
      "Copying images into  chicken_wings\n",
      "\n",
      "Copying images into  chocolate_cake\n",
      "\n",
      "Copying images into  chocolate_mousse\n",
      "\n",
      "Copying images into  churros\n",
      "\n",
      "Copying images into  clam_chowder\n",
      "\n",
      "Copying images into  club_sandwich\n",
      "\n",
      "Copying images into  crab_cakes\n",
      "\n",
      "Copying images into  creme_brulee\n",
      "\n",
      "Copying images into  croque_madame\n",
      "\n",
      "Copying images into  cup_cakes\n",
      "\n",
      "Copying images into  deviled_eggs\n",
      "\n",
      "Copying images into  donuts\n",
      "\n",
      "Copying images into  dumplings\n",
      "\n",
      "Copying images into  edamame\n",
      "\n",
      "Copying images into  eggs_benedict\n",
      "\n",
      "Copying images into  escargots\n",
      "\n",
      "Copying images into  falafel\n",
      "\n",
      "Copying images into  filet_mignon\n",
      "\n",
      "Copying images into  fish_and_chips\n",
      "\n",
      "Copying images into  foie_gras\n",
      "\n",
      "Copying images into  french_fries\n",
      "\n",
      "Copying images into  french_onion_soup\n",
      "\n",
      "Copying images into  french_toast\n",
      "\n",
      "Copying images into  fried_calamari\n",
      "\n",
      "Copying images into  fried_rice\n",
      "\n",
      "Copying images into  frozen_yogurt\n",
      "\n",
      "Copying images into  garlic_bread\n",
      "\n",
      "Copying images into  gnocchi\n",
      "\n",
      "Copying images into  greek_salad\n",
      "\n",
      "Copying images into  grilled_cheese_sandwich\n",
      "\n",
      "Copying images into  grilled_salmon\n",
      "\n",
      "Copying images into  guacamole\n",
      "\n",
      "Copying images into  gyoza\n",
      "\n",
      "Copying images into  hamburger\n",
      "\n",
      "Copying images into  hot_and_sour_soup\n",
      "\n",
      "Copying images into  hot_dog\n",
      "\n",
      "Copying images into  huevos_rancheros\n",
      "\n",
      "Copying images into  hummus\n",
      "\n",
      "Copying images into  ice_cream\n",
      "\n",
      "Copying images into  lasagna\n",
      "\n",
      "Copying images into  lobster_bisque\n",
      "\n",
      "Copying images into  lobster_roll_sandwich\n",
      "\n",
      "Copying images into  macaroni_and_cheese\n",
      "\n",
      "Copying images into  macarons\n",
      "\n",
      "Copying images into  miso_soup\n",
      "\n",
      "Copying images into  mussels\n",
      "\n",
      "Copying images into  nachos\n",
      "\n",
      "Copying images into  omelette\n",
      "\n",
      "Copying images into  onion_rings\n",
      "\n",
      "Copying images into  oysters\n",
      "\n",
      "Copying images into  pad_thai\n",
      "\n",
      "Copying images into  paella\n",
      "\n",
      "Copying images into  pancakes\n",
      "\n",
      "Copying images into  panna_cotta\n",
      "\n",
      "Copying images into  peking_duck\n",
      "\n",
      "Copying images into  pho\n",
      "\n",
      "Copying images into  pizza\n",
      "\n",
      "Copying images into  pork_chop\n",
      "\n",
      "Copying images into  poutine\n",
      "\n",
      "Copying images into  prime_rib\n",
      "\n",
      "Copying images into  pulled_pork_sandwich\n",
      "\n",
      "Copying images into  ramen\n",
      "\n",
      "Copying images into  ravioli\n",
      "\n",
      "Copying images into  red_velvet_cake\n",
      "\n",
      "Copying images into  risotto\n",
      "\n",
      "Copying images into  samosa\n",
      "\n",
      "Copying images into  sashimi\n",
      "\n",
      "Copying images into  scallops\n",
      "\n",
      "Copying images into  seaweed_salad\n",
      "\n",
      "Copying images into  shrimp_and_grits\n",
      "\n",
      "Copying images into  spaghetti_bolognese\n",
      "\n",
      "Copying images into  spaghetti_carbonara\n",
      "\n",
      "Copying images into  spring_rolls\n",
      "\n",
      "Copying images into  steak\n",
      "\n",
      "Copying images into  strawberry_shortcake\n",
      "\n",
      "Copying images into  sushi\n",
      "\n",
      "Copying images into  tacos\n",
      "\n",
      "Copying images into  takoyaki\n",
      "\n",
      "Copying images into  tiramisu\n",
      "\n",
      "Copying images into  tuna_tartare\n",
      "\n",
      "Copying images into  waffles\n",
      "Copying Done!\n"
     ]
    }
   ],
   "source": [
    "prepare_data('food-101/meta/test.txt', 'food-101/images', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m-hxgEeJVlb5",
    "outputId": "89462630-f13a-416f-b3ee-187af73c2939"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples in train folder\n",
      "75750\n",
      "Total number of samples in test folder\n",
      "25250\n"
     ]
    }
   ],
   "source": [
    "# Check how many files are in the train folder\n",
    "print(\"Total number of samples in train folder\")\n",
    "!find train -type d -or -type f -printf '.' | wc -c\n",
    "\n",
    "# Check how many files are in the test folder\n",
    "print(\"Total number of samples in test folder\")\n",
    "!find test -type d -or -type f -printf '.' | wc -c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "z0ABnDo1gJJl"
   },
   "outputs": [],
   "source": [
    "# # full image data generator\n",
    "# tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "#     featurewise_center=False,\n",
    "#     samplewise_center=False,\n",
    "#     featurewise_std_normalization=False,\n",
    "#     samplewise_std_normalization=False,\n",
    "#     zca_whitening=False,\n",
    "#     zca_epsilon=1e-06,\n",
    "#     rotation_range=0,\n",
    "#     width_shift_range=0.0,\n",
    "#     height_shift_range=0.0,\n",
    "#     brightness_range=None,\n",
    "#     shear_range=0.0,\n",
    "#     zoom_range=0.0,\n",
    "#     channel_shift_range=0.0,\n",
    "#     fill_mode=\"nearest\",\n",
    "#     cval=0.0,\n",
    "#     horizontal_flip=False,\n",
    "#     vertical_flip=False,\n",
    "#     rescale=None,\n",
    "#     preprocessing_function=None,\n",
    "#     data_format=None,\n",
    "#     validation_split=0.0,\n",
    "#     dtype=None,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def schedule(epoch):\n",
    "    if epoch < 15:\n",
    "        return .01\n",
    "    elif epoch < 28:\n",
    "        return .002\n",
    "    else:\n",
    "        return .0004\n",
    "lr_scheduler = LearningRateScheduler(schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3MjCeps4c6ku",
    "outputId": "2fe8840e-119b-4ec8-f27a-3912110856fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "Found 75750 images belonging to 101 classes.\n",
      "Found 25250 images belonging to 101 classes.\n",
      "Epoch 1/30\n",
      "INFO:tensorflow:batch_all_reduce: 192 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 192 all-reduces with algorithm = nccl, num_packs = 1\n",
      "1184/1184 [==============================] - 2248s 2s/step - loss: 3.4806 - accuracy: 0.2971 - val_loss: 55.9542 - val_accuracy: 0.0107\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 55.95423, saving model to best_model_3class.hdf5\n",
      "Epoch 2/30\n",
      "1184/1184 [==============================] - 2148s 2s/step - loss: 1.8369 - accuracy: 0.5897 - val_loss: 35.9449 - val_accuracy: 0.0096\n",
      "\n",
      "Epoch 00002: val_loss improved from 55.95423 to 35.94493, saving model to best_model_3class.hdf5\n",
      "Epoch 3/30\n",
      "1184/1184 [==============================] - 2163s 2s/step - loss: 1.5760 - accuracy: 0.6467 - val_loss: 49.3918 - val_accuracy: 0.0099\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 35.94493\n",
      "Epoch 4/30\n",
      "1184/1184 [==============================] - 2147s 2s/step - loss: 1.4156 - accuracy: 0.6835 - val_loss: 25.7419 - val_accuracy: 0.0096\n",
      "\n",
      "Epoch 00004: val_loss improved from 35.94493 to 25.74195, saving model to best_model_3class.hdf5\n",
      "Epoch 5/30\n",
      "1184/1184 [==============================] - 2150s 2s/step - loss: 1.2936 - accuracy: 0.7133 - val_loss: 9.1372 - val_accuracy: 0.0088\n",
      "\n",
      "Epoch 00005: val_loss improved from 25.74195 to 9.13715, saving model to best_model_3class.hdf5\n",
      "Epoch 6/30\n",
      "1184/1184 [==============================] - 2148s 2s/step - loss: 1.2105 - accuracy: 0.7297 - val_loss: 19.1935 - val_accuracy: 0.0088\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 9.13715\n",
      "Epoch 7/30\n",
      "1184/1184 [==============================] - 2135s 2s/step - loss: 1.1345 - accuracy: 0.7495 - val_loss: 18.3646 - val_accuracy: 0.0092\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 9.13715\n",
      "Epoch 8/30\n",
      "1184/1184 [==============================] - 2147s 2s/step - loss: 1.0757 - accuracy: 0.7625 - val_loss: 27.1629 - val_accuracy: 0.0067\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 9.13715\n",
      "Epoch 9/30\n",
      "1184/1184 [==============================] - 2146s 2s/step - loss: 1.0063 - accuracy: 0.7780 - val_loss: 14.8352 - val_accuracy: 0.0102\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 9.13715\n",
      "Epoch 10/30\n",
      "1184/1184 [==============================] - 2155s 2s/step - loss: 0.9666 - accuracy: 0.7872 - val_loss: 8.8959 - val_accuracy: 0.0093\n",
      "\n",
      "Epoch 00010: val_loss improved from 9.13715 to 8.89593, saving model to best_model_3class.hdf5\n",
      "Epoch 11/30\n",
      "1184/1184 [==============================] - 2147s 2s/step - loss: 0.9143 - accuracy: 0.7998 - val_loss: 17.8339 - val_accuracy: 0.0097\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 8.89593\n",
      "Epoch 12/30\n",
      "1184/1184 [==============================] - 2141s 2s/step - loss: 0.8711 - accuracy: 0.8084 - val_loss: 12.1905 - val_accuracy: 0.0099\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 8.89593\n",
      "Epoch 13/30\n",
      "1184/1184 [==============================] - 2133s 2s/step - loss: 0.8371 - accuracy: 0.8187 - val_loss: 18.9029 - val_accuracy: 0.0101\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 8.89593\n",
      "Epoch 14/30\n",
      "1184/1184 [==============================] - 2138s 2s/step - loss: 0.7848 - accuracy: 0.8306 - val_loss: 15.1556 - val_accuracy: 0.0093\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 8.89593\n",
      "Epoch 15/30\n",
      "1184/1184 [==============================] - 2129s 2s/step - loss: 0.7450 - accuracy: 0.8430 - val_loss: 11.3486 - val_accuracy: 0.0097\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 8.89593\n",
      "Epoch 16/30\n",
      "1184/1184 [==============================] - 2136s 2s/step - loss: 0.5800 - accuracy: 0.8837 - val_loss: 8.7057 - val_accuracy: 0.0098\n",
      "\n",
      "Epoch 00016: val_loss improved from 8.89593 to 8.70570, saving model to best_model_3class.hdf5\n",
      "Epoch 17/30\n",
      "1184/1184 [==============================] - 2137s 2s/step - loss: 0.4475 - accuracy: 0.9097 - val_loss: 9.8425 - val_accuracy: 0.0087\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 8.70570\n",
      "Epoch 18/30\n",
      "1184/1184 [==============================] - 2147s 2s/step - loss: 0.4101 - accuracy: 0.9202 - val_loss: 9.2136 - val_accuracy: 0.0075\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 8.70570\n",
      "Epoch 19/30\n",
      "1184/1184 [==============================] - 2139s 2s/step - loss: 0.3690 - accuracy: 0.9278 - val_loss: 9.4876 - val_accuracy: 0.0090\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 8.70570\n",
      "Epoch 20/30\n",
      " 205/1184 [====>.........................] - ETA: 22:28 - loss: 0.3387 - accuracy: 0.9363"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\", \"GPU:2\", \"GPU:3\"])\n",
    "with strategy.scope():\n",
    "    # apply functions to the data to improve training\n",
    "    img_width, img_height = 299, 299\n",
    "    num_of_classes = 101\n",
    "\n",
    "    # number of epochs to train top model\n",
    "    epochs = 20\n",
    "\n",
    "    # batch size used by flow_from_directory and predict_generator, try 128 if possible\n",
    "    batch_size = 64\n",
    "    #important hyper-parameters\n",
    "    metrics = ['accuracy']\n",
    "\n",
    "    datagenTrain = ImageDataGenerator(\n",
    "        rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False, # randomly flip images\n",
    "        zoom_range=[.8, 1.2],\n",
    "        channel_shift_range=30,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range = 0.2,\n",
    "        fill_mode='nearest',\n",
    "        preprocessing_function = tf.keras.applications.inception_v3.preprocess_input) #preprocess_input is the preprocessing function used in \n",
    "                                                    #original resnet152V2 model\n",
    "\n",
    "    generatorTrain = datagenTrain.flow_from_directory(\n",
    "        'train',\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True)\n",
    "\n",
    "    # iffy about this one\n",
    "    test_datagen = ImageDataGenerator(zoom_range=[.8,1.2], horizontal_flip=True, vertical_flip=False)\n",
    "\n",
    "    validation_generator = test_datagen.flow_from_directory(\n",
    "        'test',\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n",
    "    \n",
    "    \n",
    "    resnet = InceptionV3(weights='imagenet', include_top=False, input_tensor=Input(shape=(299, 299, 3)))\n",
    "    x = resnet.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(128,activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    predictions = Dense(101,kernel_regularizer=regularizers.l2(0.005), activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=resnet.input, outputs=predictions)\n",
    "    model.compile(optimizer=SGD(lr=0.02, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    checkpointer = ModelCheckpoint(filepath='best_model_3class.hdf5', verbose=1, save_best_only=True)\n",
    "    csv_logger = CSVLogger('history_3class.log')\n",
    "\n",
    "    nb_train_samples = 75750\n",
    "    nb_validation_samples = 25250\n",
    "    batch_size = 16\n",
    "\n",
    "#     history = model.fit_generator(generatorTrain,\n",
    "#                         #steps_per_epoch = nb_train_samples // batch_size,\n",
    "#                         nb_val_samples = nb_validation_samples,\n",
    "#                         validation_data=validation_generator,\n",
    "#                         #validation_steps=nb_validation_samples // batch_size,\n",
    "#                         samples_per_epoch = nb_train_samples / 25,\n",
    "#                         epochs=30,\n",
    "#                         verbose=1,\n",
    "#                         callbacks=[csv_logger, checkpointer, lr_scheduler])\n",
    "    history = model.fit(generatorTrain,\n",
    "                        batch_size= 32,\n",
    "                        epochs = 30,\n",
    "                        validation_data = validation_generator,\n",
    "                        verbose = 1,\n",
    "                        callbacks = [csv_logger, checkpointer, lr_scheduler],\n",
    "                        )\n",
    "\n",
    "    model.save('model_trained_3class.hdf5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "food101-resnet152",
   "provenance": []
  },
  "environment": {
   "name": "tf2-gpu.2-4.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
